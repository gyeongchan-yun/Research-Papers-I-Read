# Research Papers I Read 
- This repository lists up research papers that I've read, related to my research interest.  
  
- Click a [pdf] link to see the paper.

# Big Data Framework
[1] Dean, Jeffrey, and Sanjay Ghemawat. "**MapReduce: simplified data processing on large
clusters.**" Communications of the ACM 51.1 (2008): 107-113.
[[pdf]](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)

[2] Dimopoulos, Stratos, Chandra Krintz, and Rich Wolski. "**Big data framework interference in
restricted private cloud settings.**" Big Data (Big Data), 2016 IEEE International Conference on.
IEEE, 2016. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840620)

[3] Kwak, Jaewon, et al. "**In-memory caching orchestration for hadoop.**" Cluster, Cloud and Grid
Computing (CCGrid), 2016 16th IEEE/ACM International Symposium on. IEEE, 2016.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7515674)

[4] Hwang, Eunji, et al. "**CAVA: Exploring Memory Locality for Big Data Analytics in Virtualized
Clusters.**" 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8411006)

# Task Management

[1] Petrucci, Vinicius, et al. "**Octopus-man: Qos-driven task management for heterogeneous multicores
in warehouse-scale computers.**" 2015 IEEE 21st International Symposium on High Performance Computer
Architecture (HPCA). IEEE, 2015.
[[pdf]](http://homes.dcc.ufba.br/~petrucci/papers/petrucci15hpca.pdf)

# System with AI

## Machine & Deep Learning 

[1] Hashemi, Milad, et al. "**Learning Memory Access Patterns.**" arXiv preprint arXiv:1803.02329
(2018). [[pdf]](https://arxiv.org/pdf/1803.02329.pdf)

## Reinforcement Learning 

[1] Mao, Hongzi, et al. "**Resource management with deep reinforcement learning.**" Proceedings of the
15th ACM Workshop on Hot Topics in Networks. ACM, 2016.
[[pdf]](http://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf)

[2] Ipek, Engin, et al. "**Self-optimizing memory controllers: A reinforcement learning approach.**" ACM
SIGARCH Computer Architecture News. Vol. 36. No. 3. IEEE Computer Society, 2008.
[[pdf]](https://users.ece.cmu.edu/~omutlu/pub/rlmc_isca08.pdf)

[3] Baker, Bowen, et al. "**Designing neural network architectures using reinforcement learning.**"
arXiv preprint arXiv:1611.02167 (2016). [[pdf]](https://arxiv.org/pdf/1611.02167.pdf)

[4] Nishtala, Rajiv, et al. "**Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads.**"
High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on. IEEE, 2017.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7920843)

[5] Mirhoseini, Azalia, et al. "**Device placement optimization with reinforcement learning.**"
Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
[[pdf]](https://arxiv.org/pdf/1706.04972.pdf)

# System for Deep Learning

## Basic Distributed Training System

[1] Chilimbi, Trishul, et al. "**Project adam: Building an efficient and scalable deep learning training system.**"
11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.
[[pdf]](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf)

[2] Keuper, Janis, and Franz-Josef Preundt. 
"**Distributed training of deep neural networks: Theoretical and practical limits of parallel scalability.**" 
2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC). IEEE, 2016.
[[pdf]](https://arxiv.org/pdf/1609.06870.pdf)

## ML Framework

[1] Abadi, Mart√≠n, et al. "**Tensorflow: A system for large-scale machine learning.**" 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16). 2016.
[[pdf]](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)

[2] Girija, Sanjay Surendranath. "**Tensorflow: Large-scale machine learning on heterogeneous
distributed systems.**" (2016). [[pdf]](https://arxiv.org/pdf/1603.04467.pdf)

## Parallelism

[1] Huang, Yanping, et al. "**Gpipe: Efficient training of giant neural networks using pipeline parallelism.**" Advances in neural information processing systems. 2019.
[[pdf]](https://papers.nips.cc/paper/8305-gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism.pdf)

[2] Narayanan, Deepak, et al. "**PipeDream: generalized pipeline parallelism for DNN training.**" 
Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP 19). ACM, 2019.
[[pdf]](https://www.pdl.cmu.edu/PDL-FTP/BigLearning/sosp19-final271.pdf)

[3] Hegde, Vishakh, and Sheema Usmani. "**Parallel and distributed deep learning.**" (2016).
[[pdf]](https://web.stanford.edu/~rezab/classes/cme323/S16/projects_reports/hedge_usmani.pdf)

[4] Jia, Zhihao, Matei Zaharia, and Alex Aiken. "**Beyond data and model parallelism for deep neural networks.**" 
In proceedings of the conference on Systems and Machine Learning (SysML 2019). 2019.
[[pdf]](https://www.sysml.cc/doc/2019/16.pdf)

[5] Ono, Junya, Masao Utiyama, and Eiichiro Sumita. "**Hybrid Data-Model Parallel Training for Sequence-to-Sequence Recurrent Neural Network Machine Translation.**" arXiv preprint arXiv:1909.00562 (2019).
[[pdf]](https://arxiv.org/pdf/1909.00562.pdf)

## Microservice

[1] Crankshaw, Daniel, et al. "**Clipper: A Low-Latency Online Prediction Serving System.**" NSDI. 2017.
[[pdf]](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)

[2] Moritz, Philipp, et al. "**Ray: A Distributed Framework for Emerging AI Applications.**" 13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-moritz.pdf)

## GPU Memory Management

[1] Rhu, Minsoo, et al. "**vDNN: Virtualized deep neural networks for scalable, memory-efficient
neural network design.**" The 49th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE
Press, 2016. [[pdf]](https://arxiv.org/pdf/1602.08124.pdf)

[2] Kim, Youngrang, et al. "**Efficient Multi-GPU Memory Management for Deep Learning
Acceleration.**" 2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS* W).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599530)

[3] Meng, Chen, et al. "**Training deeper models by GPU memory optimization on TensorFlow.**" Proc. of
ML Systems Workshop in NIPS. 2017. [[pdf]](http://learningsys.org/nips17/assets/papers/paper_18.pdf)

[4] Li, Chen, et al. "**A Framework for Memory Oversubscription Management in Graphics Processing Units.**" 
Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). ACM, 2019.
[[pdf]](https://people.inf.ethz.ch/omutlu/pub/ETC-memory-oversubscription-management-framework-for-GPUs_asplos19.pdf)

[5] Jain, Animesh, et al. "**Gist: Efficient data encoding for deep neural network training.**"
2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2018.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416872)

[6] Yu, Peifeng, and Mosharaf Chowdhury. "**Salus: Fine-grained gpu sharing primitives for deep learning applications.**" 
Proceedings of Machine Learning and Systems 2020 (MLSys 2020).
[[pdf]](https://www.mosharaf.com/wp-content/uploads/salus-mlsys20.pdf)

## Parameter Server

[1] Li, Mu, et al. "**Scaling distributed machine learning with the parameter server.**" 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.
[[pdf]](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)

[2] Jiang, Jiawei, et al. "**Heterogeneity-aware distributed parameter servers.**"
Proceedings of the 2017 ACM International Conference on Management of Data. ACM, 2017.
[[pdf]](http://net.pku.edu.cn/~cuibin/Papers/2017%20sigmod.pdf)

[3] Cui, Henggang, et al. "**Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server.**"
Proceedings of the Eleventh European Conference on Computer Systems (EuroSys). ACM, 2016.
[[pdf]](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)

[4] Park, Jay H., et al. "**Accelerated Training for CNN Distributed Deep Learning through Automatic Resource-Aware Layer Placement.**" 
arXiv preprint arXiv:1901.05803 (2019).
[[pdf]](https://arxiv.org/pdf/1901.05803.pdf)

[5] Chen, Chen, et al. "**Fast distributed deep learning via worker-adaptive batch sizing.**" 
Proceedings of the ACM Symposium on Cloud Computing (SoCC). 2018.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3267809.3275463)

## Cluster Scheduling

[1] Bao, Yixin, et al. "**Online Job Scheduling in Distributed Machine Learning Clusters.**" IEEE
INFOCOM 2018-IEEE Conference on Computer Communications. IEEE, 2018.
[[pdf]](https://arxiv.org/pdf/1801.00936.pdf)

[2] Xiao, Wencong, et al. "**Gandiva: Introspective cluster scheduling for deep learning.**" 
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-xiao.pdf)

[3] Chaudhary, Shubham, et al. "**Balancing efficiency and fairness in heterogeneous GPU clusters for deep learning.**" Proceedings of the Fifteenth European Conference on Computer Systems (Eurosys 20). 2020.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3342195.3387555)

[4] Peng, Yanghua, et al. "**Optimus: an efficient dynamic resource scheduler for deep learning clusters.**" Proceedings of the 13th EuroSys Conference. 2018.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3190508.3190517)

[5] Gu, Juncheng, et al. "**Tiresias: A GPU cluster manager for distributed deep learning.**" 
16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19). 2019.
[[pdf]](https://www.usenix.org/system/files/nsdi19-gu.pdf)

[6] Le, Tan N., et al. "**AlloX: compute allocation in hybrid clusters.**" Proceedings of the 15th European Conference on Computer Systems (Eurosys 20). 2020.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3342195.3387547)

[7] Han, Jingoo, et al. "**MARBLE: A Multi-GPU Aware Job Scheduler for Deep Learning on HPC Systems.**" 
2020 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing (CCGRID). IEEE, 2020.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9139710)

## Synchronization (Convergence)

[1] Zhang, Chengliang, et al. "**Stay Fresh: Speculative Synchronization for Fast Distributed Machine Learning.**"
2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS). IEEE, 2018.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416283)

[2] Dean, Jeffrey, et al. "**Large Scale Distributed Deep Networks.**" 
Advances in neural information processing systems (NIPS). 2012.
[[pdf]](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)

[3] Goyal, Priya, et al. "**Accurate, large minibatch sgd: Training imagenet in 1 hour.**" arXiv preprint arXiv:1706.02677 (2017). [[pdf]](https://arxiv.org/pdf/1706.02677.pdf%5B3%5D%20ImageNet)

[4] Bottou, L√©on, and Olivier Bousquet. "**The tradeoffs of large scale learning.**" Advances in neural information processing systems (NIPS). 2008. [[pdf]](http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf)

[5] Chen, Jianmin, et al. "**Revisiting distributed synchronous SGD.**" arXiv preprint arXiv:1604.00981 (2016).
[[pdf]](https://arxiv.org/pdf/1604.00981.pdf)

## Communication

[1] Zhang, Hao, et al. "**Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters.**"
2017 USENIX Annual Technical Conference (USENIX ATC 17). 2017.
[[pdf]](https://www.usenix.org/system/files/conference/atc17/atc17-zhang.pdf)

[2] Sergeev, Alexander, and Mike Del Balso. "**Horovod: fast and easy distributed deep learning in TensorFlow.**"
arXiv preprint arXiv:1802.05799 (2018).
[[pdf]](https://arxiv.org/pdf/1802.05799.pdf)

[3] Kim, Soojeong, et al. "**Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks.**"
Proceedings of the Fourteenth EuroSys Conference 2019. ACM, 2019.
[[pdf]](https://spl.snu.ac.kr/wp-content/uploads/2012/07/parallax.pdf)

[4] Xue, Jilong, et al. "**Fast Distributed Deep Learning over RDMA.**"
Proceedings of the Fourteenth EuroSys Conference 2019. ACM, 2019.
[[pdf]](https://readingxtra.github.io/docs/RMDA/eurosys19-xue-jilong.pdf)

[5] Peng, Yanghua, et al. "**A generic communication scheduler for distributed DNN training acceleration.**" 
the 27th ACM Symposium on Operating Systems Principles (SOSP 19)
[[pdf]](https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf)

## Performance Metric (Benchmark)

[1] Coleman, Cody, et al. "**Dawnbench: An end-to-end deep learning benchmark and competition.**"
Advances in neural information processing systems (NIPS). 2017.
[[pdf]](https://ddkang.github.io/papers/dawnbench-nips17.pdf)

[2] Mattson, Peter, et al. "**Mlperf training benchmark.**" arXiv preprint arXiv:1910.01500 (2019).
[[pdf]](https://arxiv.org/pdf/1910.01500.pdf)

## Data Partitioning

[1] Wei, Kai, et al. "**How to intelligently distribute training data to multiple compute nodes: Distributed machine learning via submodular partitioning.**" 
Neural Information Processing Society (NIPS) Workshop, Montreal, Canada. 2015.
[[pdf]](http://learningsys.org/papers/LearningSys_2015_paper_16.pdf)

## Code Optimization

[1] Chen, Tianqi, et al. "**TVM: An automated end-to-end optimizing compiler for deep learning.**" 
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-chen.pdf)

## Model Testing

[1] Pei, Kexin, et al. "**Deepxplore: Automated whitebox testing of deep learning systems.**" 
Proceedings of the 26th Symposium on Operating Systems Principles (SOSP 17). ACM, 2017.
[[pdf]](http://www.cs.columbia.edu/~junfeng/papers/deepxplore-sosp17.pdf)

[2] Lee, Yunseong, et al. "**PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems.**"
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-lee.pdf)

## Serving

[1] Zhang, Minjia, et al. "**Deepcpu: Serving rnn-based deep learning models 10x faster.**" 
2018 USENIX Annual Technical Conference (USENIX ATC 18). 2018.
[[pdf]](https://www.usenix.org/system/files/conference/atc18/atc18-zhang-minjia.pdf)

## Federated Learning

[1] Bonawitz, Keith, et al. "**Towards federated learning at scale: System design.**"
arXiv preprint arXiv:1902.01046 (2019). 
[[pdf]](https://arxiv.org/pdf/1902.01046.pdf)

[2] McMahan, H. Brendan, et al. "**Communication-efficient learning of deep networks from decentralized data.**" 
arXiv preprint arXiv:1602.05629 (2016).
[[pdf]](https://arxiv.org/pdf/1602.05629.pdf)

## Compression (Pruning, Quantization, Precision)

[1] Han, Song, Huizi Mao, and William J. Dally. "**Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.**" 
arXiv preprint arXiv:1510.00149 (2015).
[[pdf]](https://arxiv.org/pdf/1510.00149)

[2] Alistarh, Dan, et al. "**QSGD: Communication-efficient SGD via gradient quantization and encoding.**"
Advances in Neural Information Processing Systems. 2017. 
[[pdf]](https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf)

[3] Zhou, Aojun, et al. "**Incremental network quantization: Towards lossless cnns with low-precision weights.**" arXiv preprint arXiv:1702.03044 (2017).
[[pdf]](https://arxiv.org/pdf/1702.03044.pdf)

[4] Wen, Wei, et al. "**Learning structured sparsity in deep neural networks.**" 
Advances in neural information processing systems. 2016.
[[pdf]](https://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf)

[5] Micikevicius, Paulius, et al. "**Mixed precision training.**" arXiv preprint arXiv:1710.03740 (2017).
[[pdf]](https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82)

[6] Luo, Jian-Hao, and Jianxin Wu. "**Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference.**" arXiv preprint arXiv:1805.08941 (2018).
[[pdf]](https://arxiv.org/pdf/1805.08941.pdf)

## Multi-task Learning

[1] Liu, Sulin, Sinno Jialin Pan, and Qirong Ho. "**Distributed multi-task relationship learning.**" 
Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2017.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3097983.3098136)

## Decentralized Training

[1] Lian, Xiangru, et al. "**Asynchronous decentralized parallel stochastic gradient descent.**" 
arXiv preprint arXiv:1710.06952 (2017).
[[pdf]](http://proceedings.mlr.press/v80/lian18a/lian18a.pdf)

[2] Lian, Xiangru, et al. "**Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent.**" 
Advances in Neural Information Processing Systems (NIPS). 2017.
[[pdf]](https://papers.nips.cc/paper/7117-can-decentralized-algorithms-outperform-centralized-algorithms-a-case-study-for-decentralized-parallel-stochastic-gradient-descent.pdf)

[3] Luo, Qinyi, et al. "**Hop: Heterogeneity-aware decentralized training.**" 
Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). 2019.
[[pdf]](http://alchem.usc.edu/~qinyi/download/hop.pdf)

[4] Kadav, Asim, and Erik Kruus. "**ASAP: asynchronous approximate data-parallel computation.**"
arXiv preprint arXiv:1612.08608 (2016).
[[pdf]](https://arxiv.org/pdf/1612.08608.pdf)

[5] Luo, Qinyi, et al. "**Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training.**" 
Proceedings of the 24th ASPLOS. 2020.
[[pdf]](https://dl.acm.org/doi/pdf/10.1145/3373376.3378499)

## Hyper-parameter Optimization

[1] Ahnjae Shin, et al. "**Stage-based hyper-parameter optimization for deep learning**" Systems for ML Workshop at NeurIPS 2019.
[[pdf]](https://spl.snu.ac.kr/wp-content/uploads/2012/07/hippo.pdf)

[2] Smith, Samuel L., et al. "**Don't decay the learning rate, increase the batch size.**" ICLR. 2018.
[[pdf]](https://arxiv.org/pdf/1711.00489.pdf?source=post_page---------------------------)

## Ensemble Training

[1] Pittman, Randall, et al. "**Exploring flexible communications for streamlining DNN ensemble training pipelines.**" 
SC 18: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2018.
[[pdf]](https://repository.lib.ncsu.edu/bitstream/handle/1840.20/35263/etd.pdf?sequence=1&isAllowed=y)

[2] Guan, Hui, et al. "**FLEET: Flexible Efficient Ensemble Training for Heterogeneous Deep Neural Networks.**" 
Proceedings of Machine Learning and Systems 2020 (MLSys 2020).
[[pdf]](https://proceedings.mlsys.org/static/paper_files/mlsys/2020/98-Paper.pdf)

## Resource Management

[1] Or, Andrew, Haoyu Zhang, and Michael J. Freedman. "**Resource Elasticity in Distributed Deep Learning.**"
Proceedings of Machine Learning and Systems 2020 (MLSys 2020).
[[pdf]](https://proceedings.mlsys.org/static/paper_files/mlsys/2020/168-Paper.pdf)

## Storage

[1] Eisenman, Assaf, et al. "**Bandana: Using non-volatile memory for storing deep learning models.**"
Proceedings of Machine Learning and Systems 2019 (MLSys 2019).
[[pdf]](https://proceedings.mlsys.org/book/2019/file/34173cb38f07f89ddbebc2ac9128303f-Paper.pdf)

# System for Reinforcement Learning

## Parallel Method

[1] Mnih, Volodymyr, et al. "**Asynchronous methods for deep reinforcement learning.**" ICML. 2016.
[[pdf]](http://proceedings.mlr.press/v48/mniha16.pdf)

[2] Nair, Arun, et al. "**Massively parallel methods for deep reinforcement learning.**" arXiv preprint arXiv:1507.04296 (2015).
[[pdf]](https://arxiv.org/pdf/1507.04296.pdf)
