# Research Papers I Read 
- This repository lists up research papers that I've read, related to my research interest.  
  
- Click a [pdf] link to see the paper.

# Big Data Framework
[1] Dean, Jeffrey, and Sanjay Ghemawat. "**MapReduce: simplified data processing on large
clusters.**" Communications of the ACM 51.1 (2008): 107-113.
[[pdf]](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)

[2] Dimopoulos, Stratos, Chandra Krintz, and Rich Wolski. "**Big data framework interference in
restricted private cloud settings.**" Big Data (Big Data), 2016 IEEE International Conference on.
IEEE, 2016. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840620)

[3] Kwak, Jaewon, et al. "**In-memory caching orchestration for hadoop.**" Cluster, Cloud and Grid
Computing (CCGrid), 2016 16th IEEE/ACM International Symposium on. IEEE, 2016.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7515674)

[4] Hwang, Eunji, et al. "**CAVA: Exploring Memory Locality for Big Data Analytics in Virtualized
Clusters.**" 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8411006)

# Task Management

[1] Petrucci, Vinicius, et al. "**Octopus-man: Qos-driven task management for heterogeneous multicores
in warehouse-scale computers.**" 2015 IEEE 21st International Symposium on High Performance Computer
Architecture (HPCA). IEEE, 2015.
[[pdf]](http://homes.dcc.ufba.br/~petrucci/papers/petrucci15hpca.pdf)

# System with AI

## Machine & Deep Learning 

[1] Hashemi, Milad, et al. "**Learning Memory Access Patterns.**" arXiv preprint arXiv:1803.02329
(2018). [[pdf]](https://arxiv.org/pdf/1803.02329.pdf)

## Reinforcement Learning 

[1] Mao, Hongzi, et al. "**Resource management with deep reinforcement learning.**" Proceedings of the
15th ACM Workshop on Hot Topics in Networks. ACM, 2016.
[[pdf]](http://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf)

[2] Ipek, Engin, et al. "**Self-optimizing memory controllers: A reinforcement learning approach.**" ACM
SIGARCH Computer Architecture News. Vol. 36. No. 3. IEEE Computer Society, 2008.
[[pdf]](https://users.ece.cmu.edu/~omutlu/pub/rlmc_isca08.pdf)

[3] Baker, Bowen, et al. "**Designing neural network architectures using reinforcement learning.**"
arXiv preprint arXiv:1611.02167 (2016). [[pdf]](https://arxiv.org/pdf/1611.02167.pdf)

[4] Nishtala, Rajiv, et al. "**Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads.**"
High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on. IEEE, 2017.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7920843)

[5] Mirhoseini, Azalia, et al. "**Device placement optimization with reinforcement learning.**"
Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
[[pdf]](https://arxiv.org/pdf/1706.04972.pdf)

# System for Deep Learning

## ML Framework

[1] Abadi, Mart√≠n, et al. "**Tensorflow: A system for large-scale machine learning.**" 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16). 2016.
[[pdf]](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)

[2] Girija, Sanjay Surendranath. "**Tensorflow: Large-scale machine learning on heterogeneous
distributed systems.**" (2016). [[pdf]](https://arxiv.org/pdf/1603.04467.pdf)

## Parallelism

[1] Huang, Yanping, et al. "**GPipe: Efficient Training of Giant Neural Networks using Pipeline
Parallelism.**" arXiv preprint arXiv:1811.06965 (2018). [[pdf]](https://arxiv.org/pdf/1811.06965.pdf)

[2] Harlap, Aaron, et al. "**Pipedream: Fast and efficient pipeline parallel dnn training.**" arXiv
preprint arXiv:1806.03377 (2018). [[pdf]](https://arxiv.org/pdf/1806.03377.pdf)

[3] Hegde, Vishakh, and Sheema Usmani. "**Parallel and distributed deep learning.**" (2016).
[[pdf]](https://web.stanford.edu/~rezab/classes/cme323/S16/projects_reports/hedge_usmani.pdf)

## Microservice

[1] Crankshaw, Daniel, et al. "**Clipper: A Low-Latency Online Prediction Serving System.**" NSDI. 2017.
[[pdf]](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)

[2] Moritz, Philipp, et al. "**Ray: A Distributed Framework for Emerging AI Applications.**" 13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-moritz.pdf)

## GPU Memory Management

[1] Rhu, Minsoo, et al. "**vDNN: Virtualized deep neural networks for scalable, memory-efficient
neural network design.**" The 49th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE
Press, 2016. [[pdf]](https://arxiv.org/pdf/1602.08124.pdf)

[2] Kim, Youngrang, et al. "**Efficient Multi-GPU Memory Management for Deep Learning
Acceleration.**" 2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS* W).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599530)

[3] Meng, Chen, et al. "**Training deeper models by GPU memory optimization on TensorFlow.**" Proc. of
ML Systems Workshop in NIPS. 2017. [[pdf]](http://learningsys.org/nips17/assets/papers/paper_18.pdf)

## Parameter Server

[1] Li, Mu, et al. "**Scaling distributed machine learning with the parameter server.**" 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.
[[pdf]](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)

## Scheduling

[1] Bao, Yixin, et al. "**Online Job Scheduling in Distributed Machine Learning Clusters.**" IEEE
INFOCOM 2018-IEEE Conference on Computer Communications. IEEE, 2018.
[[pdf]](https://arxiv.org/pdf/1801.00936.pdf)
