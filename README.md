# Research Papers I Read 
- This repository lists up research papers that I've read, related to my research interest.  
  
- Click a [pdf] link to see the paper.

# Big Data Framework
[1] Dean, Jeffrey, and Sanjay Ghemawat. "**MapReduce: simplified data processing on large
clusters.**" Communications of the ACM 51.1 (2008): 107-113.
[[pdf]](https://www.usenix.org/legacy/events/osdi04/tech/full_papers/dean/dean.pdf)

[2] Dimopoulos, Stratos, Chandra Krintz, and Rich Wolski. "**Big data framework interference in
restricted private cloud settings.**" Big Data (Big Data), 2016 IEEE International Conference on.
IEEE, 2016. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840620)

[3] Kwak, Jaewon, et al. "**In-memory caching orchestration for hadoop.**" Cluster, Cloud and Grid
Computing (CCGrid), 2016 16th IEEE/ACM International Symposium on. IEEE, 2016.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7515674)

[4] Hwang, Eunji, et al. "**CAVA: Exploring Memory Locality for Big Data Analytics in Virtualized
Clusters.**" 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8411006)

# Task Management

[1] Petrucci, Vinicius, et al. "**Octopus-man: Qos-driven task management for heterogeneous multicores
in warehouse-scale computers.**" 2015 IEEE 21st International Symposium on High Performance Computer
Architecture (HPCA). IEEE, 2015.
[[pdf]](http://homes.dcc.ufba.br/~petrucci/papers/petrucci15hpca.pdf)

# System with AI

## Machine & Deep Learning 

[1] Hashemi, Milad, et al. "**Learning Memory Access Patterns.**" arXiv preprint arXiv:1803.02329
(2018). [[pdf]](https://arxiv.org/pdf/1803.02329.pdf)

## Reinforcement Learning 

[1] Mao, Hongzi, et al. "**Resource management with deep reinforcement learning.**" Proceedings of the
15th ACM Workshop on Hot Topics in Networks. ACM, 2016.
[[pdf]](http://people.csail.mit.edu/alizadeh/papers/deeprm-hotnets16.pdf)

[2] Ipek, Engin, et al. "**Self-optimizing memory controllers: A reinforcement learning approach.**" ACM
SIGARCH Computer Architecture News. Vol. 36. No. 3. IEEE Computer Society, 2008.
[[pdf]](https://users.ece.cmu.edu/~omutlu/pub/rlmc_isca08.pdf)

[3] Baker, Bowen, et al. "**Designing neural network architectures using reinforcement learning.**"
arXiv preprint arXiv:1611.02167 (2016). [[pdf]](https://arxiv.org/pdf/1611.02167.pdf)

[4] Nishtala, Rajiv, et al. "**Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads.**"
High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on. IEEE, 2017.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7920843)

[5] Mirhoseini, Azalia, et al. "**Device placement optimization with reinforcement learning.**"
Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.
[[pdf]](https://arxiv.org/pdf/1706.04972.pdf)

# System for Deep Learning

## Basic Distributed Training System

[1] Chilimbi, Trishul, et al. "**Project adam: Building an efficient and scalable deep learning training system.**"
11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.
[[pdf]](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-chilimbi.pdf)

## ML Framework

[1] Abadi, Martín, et al. "**Tensorflow: A system for large-scale machine learning.**" 12th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 16). 2016.
[[pdf]](https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf)

[2] Girija, Sanjay Surendranath. "**Tensorflow: Large-scale machine learning on heterogeneous
distributed systems.**" (2016). [[pdf]](https://arxiv.org/pdf/1603.04467.pdf)

## Parallelism

[1] Huang, Yanping, et al. "**GPipe: Efficient Training of Giant Neural Networks using Pipeline
Parallelism.**" arXiv preprint arXiv:1811.06965 (2018). [[pdf]](https://arxiv.org/pdf/1811.06965.pdf)

[2] Narayanan, Deepak, et al. "**PipeDream: generalized pipeline parallelism for DNN training.**" 
Proceedings of the 27th ACM Symposium on Operating Systems Principles (SOSP 19). ACM, 2019.
[[pdf]](https://www.pdl.cmu.edu/PDL-FTP/BigLearning/sosp19-final271.pdf)

[3] Hegde, Vishakh, and Sheema Usmani. "**Parallel and distributed deep learning.**" (2016).
[[pdf]](https://web.stanford.edu/~rezab/classes/cme323/S16/projects_reports/hedge_usmani.pdf)

[4] Jia, Zhihao, Matei Zaharia, and Alex Aiken. "**Beyond data and model parallelism for deep neural networks.**" 
In proceedings of the conference on Systems and Machine Learning (SysML 2019). 2019.
[[pdf]](https://www.sysml.cc/doc/2019/16.pdf)

## Microservice

[1] Crankshaw, Daniel, et al. "**Clipper: A Low-Latency Online Prediction Serving System.**" NSDI. 2017.
[[pdf]](https://www.usenix.org/system/files/conference/nsdi17/nsdi17-crankshaw.pdf)

[2] Moritz, Philipp, et al. "**Ray: A Distributed Framework for Emerging AI Applications.**" 13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-moritz.pdf)

## GPU Memory Management

[1] Rhu, Minsoo, et al. "**vDNN: Virtualized deep neural networks for scalable, memory-efficient
neural network design.**" The 49th Annual IEEE/ACM International Symposium on Microarchitecture. IEEE
Press, 2016. [[pdf]](https://arxiv.org/pdf/1602.08124.pdf)

[2] Kim, Youngrang, et al. "**Efficient Multi-GPU Memory Management for Deep Learning
Acceleration.**" 2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS* W).
IEEE, 2018. [[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599530)

[3] Meng, Chen, et al. "**Training deeper models by GPU memory optimization on TensorFlow.**" Proc. of
ML Systems Workshop in NIPS. 2017. [[pdf]](http://learningsys.org/nips17/assets/papers/paper_18.pdf)

[4] Li, Chen, et al. "**A Framework for Memory Oversubscription Management in Graphics Processing Units.**" 
Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). ACM, 2019.
[[pdf]](https://people.inf.ethz.ch/omutlu/pub/ETC-memory-oversubscription-management-framework-for-GPUs_asplos19.pdf)

[5] Jain, Animesh, et al. "**Gist: Efficient data encoding for deep neural network training.**"
2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2018.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416872)

## Parameter Server

[1] Li, Mu, et al. "**Scaling distributed machine learning with the parameter server.**" 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 14). 2014.
[[pdf]](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf)

[2] Jiang, Jiawei, et al. "**Heterogeneity-aware distributed parameter servers.**"
Proceedings of the 2017 ACM International Conference on Management of Data. ACM, 2017.
[[pdf]](http://net.pku.edu.cn/~cuibin/Papers/2017%20sigmod.pdf)

[3] Cui, Henggang, et al. "**Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server.**"
Proceedings of the Eleventh European Conference on Computer Systems (EuroSys). ACM, 2016.
[[pdf]](https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf)

[4] Park, Jay H., et al. "**Accelerated Training for CNN Distributed Deep Learning through Automatic Resource-Aware Layer Placement.**" 
arXiv preprint arXiv:1901.05803 (2019).
[[pdf]](https://arxiv.org/pdf/1901.05803.pdf)

## Cluster Scheduling

[1] Bao, Yixin, et al. "**Online Job Scheduling in Distributed Machine Learning Clusters.**" IEEE
INFOCOM 2018-IEEE Conference on Computer Communications. IEEE, 2018.
[[pdf]](https://arxiv.org/pdf/1801.00936.pdf)

[2] Xiao, Wencong, et al. "**Gandiva: Introspective cluster scheduling for deep learning.**" 
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-xiao.pdf)

## Synchronization (Convergence)

[1] Zhang, Chengliang, et al. "**Stay Fresh: Speculative Synchronization for Fast Distributed Machine Learning.**"
2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS). IEEE, 2018.
[[pdf]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416283)

[2] Dean, Jeffrey, et al. "**Large Scale Distributed Deep Networks.**" 
Advances in neural information processing systems (NIPS). 2012.
[[pdf]](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)

[3] Goyal, Priya, et al. "**Accurate, large minibatch sgd: Training imagenet in 1 hour.**" arXiv preprint arXiv:1706.02677 (2017). [[pdf]](https://arxiv.org/pdf/1706.02677.pdf%5B3%5D%20ImageNet)

[4] Bottou, Léon, and Olivier Bousquet. "**The tradeoffs of large scale learning.**" Advances in neural information processing systems (NIPS). 2008. [[pdf]](http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf)

[5] Lian, Xiangru, et al. "**Asynchronous decentralized parallel stochastic gradient descent.**" arXiv preprint arXiv:1710.06952 (2017).

## Communication

[1] Zhang, Hao, et al. "**Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters.**"
2017 USENIX Annual Technical Conference (USENIX ATC 17). 2017.
[[pdf]](https://www.usenix.org/system/files/conference/atc17/atc17-zhang.pdf)

[2] Sergeev, Alexander, and Mike Del Balso. "**Horovod: fast and easy distributed deep learning in TensorFlow.**"
arXiv preprint arXiv:1802.05799 (2018).
[[pdf]](https://arxiv.org/pdf/1802.05799.pdf)

[3] Kim, Soojeong, et al. "**Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks.**"
Proceedings of the Fourteenth EuroSys Conference 2019. ACM, 2019.
[[pdf]](https://spl.snu.ac.kr/wp-content/uploads/2012/07/parallax.pdf)

[4] Xue, Jilong, et al. "**Fast Distributed Deep Learning over RDMA.**"
Proceedings of the Fourteenth EuroSys Conference 2019. ACM, 2019.
[[pdf]](https://readingxtra.github.io/docs/RMDA/eurosys19-xue-jilong.pdf)

[5] Peng, Yanghua, et al. "**A generic communication scheduler for distributed DNN training acceleration.**" 
the 27th ACM Symposium on Operating Systems Principles (SOSP 19)
[[pdf]](https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf)

## Performance Metric

[1] Coleman, Cody, et al. "**Dawnbench: An end-to-end deep learning benchmark and competition.**"
Advances in neural information processing systems (NIPS). 2017.
[[pdf]](https://ddkang.github.io/papers/dawnbench-nips17.pdf)

## Data Partitioning

[1] Wei, Kai, et al. "**How to intelligently distribute training data to multiple compute nodes: Distributed machine learning via submodular partitioning.**" 
Neural Information Processing Society (NIPS) Workshop, Montreal, Canada. 2015.
[[pdf]](http://learningsys.org/papers/LearningSys_2015_paper_16.pdf)

## Code Optimization

[1] Chen, Tianqi, et al. "**TVM: An automated end-to-end optimizing compiler for deep learning.**" 
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-chen.pdf)

## Model Testing

[1] Pei, Kexin, et al. "**Deepxplore: Automated whitebox testing of deep learning systems.**" 
Proceedings of the 26th Symposium on Operating Systems Principles (SOSP 17). ACM, 2017.
[[pdf]](http://www.cs.columbia.edu/~junfeng/papers/deepxplore-sosp17.pdf)

[2] Lee, Yunseong, et al. "**PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems.**"
13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 2018.
[[pdf]](https://www.usenix.org/system/files/osdi18-lee.pdf)

## Serving

[1] Zhang, Minjia, et al. "**Deepcpu: Serving rnn-based deep learning models 10x faster.**" 
2018 USENIX Annual Technical Conference (USENIX ATC 18). 2018.
[[pdf]](https://www.usenix.org/system/files/conference/atc18/atc18-zhang-minjia.pdf)

## Federated Learning

[1] Bonawitz, Keith, et al. "**Towards federated learning at scale: System design.**"
arXiv preprint arXiv:1902.01046 (2019). 
[[pdf]](https://arxiv.org/pdf/1902.01046.pdf)

[2] McMahan, H. Brendan, et al. "**Communication-efficient learning of deep networks from decentralized data.**" 
arXiv preprint arXiv:1602.05629 (2016).
[[pdf]](https://arxiv.org/pdf/1602.05629.pdf)

## Compression (Pruning, Quantization, Precision)

[1] Han, Song, Huizi Mao, and William J. Dally. "**Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.**" 
arXiv preprint arXiv:1510.00149 (2015).
[[pdf]](https://arxiv.org/pdf/1510.00149)

[2] Alistarh, Dan, et al. "**QSGD: Communication-efficient SGD via gradient quantization and encoding.**"
Advances in Neural Information Processing Systems. 2017. 
[[pdf]](https://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf)

[3] Zhou, Aojun, et al. "**Incremental network quantization: Towards lossless cnns with low-precision weights.**" arXiv preprint arXiv:1702.03044 (2017).
[[pdf]](https://arxiv.org/pdf/1702.03044.pdf)

[4] Wen, Wei, et al. "**Learning structured sparsity in deep neural networks.**" 
Advances in neural information processing systems. 2016.
[[pdf]](https://papers.nips.cc/paper/6504-learning-structured-sparsity-in-deep-neural-networks.pdf)

[5] Micikevicius, Paulius, et al. "**Mixed precision training.**" arXiv preprint arXiv:1710.03740 (2017).
[[pdf]](https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82)

[6] Luo, Jian-Hao, and Jianxin Wu. "**Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference.**" arXiv preprint arXiv:1805.08941 (2018).
[[pdf]](https://arxiv.org/pdf/1805.08941.pdf)
